# Задача

✅ Разработать многопоточный веб-скрейпер с очередью задач и ограничением параллельных запросов

- Получает на вход список URL и скачивает заголовки (\<title>) и описание (\<meta name="description">).
- Сохраняет данные в CSV-файл (date, url, status_code, title, description).

Дополнительные требования:

- Ограничить количество одновременных запросов
- Логировать ошибки (в том числе 4XX ответы сайта).
- Добавить таймаут для запросов.
- Добавить повторные попытки для временно недоступных сайтов (до 3 раз).
- Ограничение одновременных запросов и таймаут вынести в конфигурацию

# ENV

| переменная         |            что делает             | дефолтное значение |
|--------------------|:---------------------------------:|:------------------:|
| RETRY_COUNT        |   количество повторных попыток    |         3          |
| TIMEOUT            |             таймаут              |        10s         |
| PARALLEL_REQ_COUNT | количество одновременных запросов |         10         |
| CSV_FILENAME       |           имя CSV-файла           |     tasks.csv      |

# Usage

```bash
./main https://ya.ru https://yandex.ru https://google.com
```